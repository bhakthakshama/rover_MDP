{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Consider a self-powered rover that operates on a slope. The rover can be in one of the following four states: (1) low (2) medium (3) high (4) top. The rover has a motor that can spin its wheel slowly at the expense of 4 unit of energy per time step or rapidly at the expense of 12 units of energy per time step. If the motor spins the wheel slowly, with probability 0.3 it moves to the next higher state in one time step, and with probability 0.7, it slides all the way down the slope to the low state. On the other hand, if the motor spins the wheel rapidly, with probability 0.5 it moves to the next higher state in one time step, and with probability 0.5, it slides all the way down the slope to the low state. The rover’s motion terminates once it reaches the top state. The rover is initially low on the slope and aims to reach the top with minimum energy consumption. Build an MDP class to simulate the reward sequence for the rover.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gupT2CKv77ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KSZTAh_Ghbdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for the rover Markov Decision Process (MDP)\n",
        "class RoverMDP:\n",
        "\n",
        "    def __init__(self):\n",
        "        # State space of the MDP\n",
        "        self.states = [\"low\", \"medium\", \"high\", \"top\"]\n",
        "\n",
        "    # Starting state of the MDP\n",
        "    def startState(self):\n",
        "        return self.states[0]\n",
        "\n",
        "    # Check if the MDP has ended\n",
        "    def isEnd(self, state):\n",
        "        return state == self.states[3]\n",
        "\n",
        "    # Possible actions from a given state\n",
        "    def actions(self, state):\n",
        "        return [\"slow\", \"rapid\"]\n",
        "\n",
        "    # Return the list of possible new states, the corresponding probabilities, and the corresponding rewards\n",
        "    def NewStateProbReward(self, state, action):\n",
        "        newStateProbReward_list = []\n",
        "\n",
        "        if self.isEnd(state):\n",
        "            # Terminal state, no transitions\n",
        "            return [(state, 1.0, 0)]\n",
        "\n",
        "        if action in self.actions(state):\n",
        "            if state == \"low\":\n",
        "                if action == \"slow\":\n",
        "                    newStateProbReward_list.append((\"low\", 0.7, -4))  # Slides back to low\n",
        "                    newStateProbReward_list.append((\"medium\", 0.3, -4))  # Moves to medium\n",
        "                elif action == \"rapid\":\n",
        "                    newStateProbReward_list.append((\"low\", 0.5, -12))  # Slides back to low\n",
        "                    newStateProbReward_list.append((\"medium\", 0.5, -12))  # Moves to medium\n",
        "\n",
        "            elif state == \"medium\":\n",
        "                if action == \"slow\":\n",
        "                    newStateProbReward_list.append((\"low\", 0.7, -4))  # Slides back to low\n",
        "                    newStateProbReward_list.append((\"high\", 0.3, -4))  # Moves to high\n",
        "                elif action == \"rapid\":\n",
        "                    newStateProbReward_list.append((\"low\", 0.5, -12))  # Slides back to low\n",
        "                    newStateProbReward_list.append((\"high\", 0.5, -12))  # Moves to high\n",
        "\n",
        "            elif state == \"high\":\n",
        "                if action == \"slow\":\n",
        "                    newStateProbReward_list.append((\"low\", 0.7, -4))  # Slides back to low\n",
        "                    newStateProbReward_list.append((\"top\", 0.3, -4))  # Moves to top\n",
        "                elif action == \"rapid\":\n",
        "                    newStateProbReward_list.append((\"low\", 0.5, -12))  # Slides back to low\n",
        "                    newStateProbReward_list.append((\"top\", 0.5, -12))  # Moves to top\n",
        "\n",
        "        return newStateProbReward_list\n",
        "\n",
        "    # Reward discounting\n",
        "    def discount(self):\n",
        "        return 0.95"
      ],
      "metadata": {
        "id": "KlWvWekghsT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dice game MDP object\n",
        "roverMDP = RoverMDP ()"
      ],
      "metadata": {
        "id": "dA9EX5NOpZAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(roverMDP.states)\n",
        "print(roverMDP.actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "103Rns92NT5i",
        "outputId": "ecfc52c6-c69c-42cc-d977-2d53642d6956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['low', 'medium', 'high', 'top']\n",
            "<bound method RoverMDP.actions of <__main__.RoverMDP object at 0x78199428df90>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the possible actions available from state 0 which is the 'in' state?\n",
        "roverMDP.actions (0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcwzwn9jpnbq",
        "outputId": "554fe4f2-b619-4b92-d18d-7b2a02d614da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['slow', 'rapid']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the possible actions available from state 3 which is the 'out' state ?\n",
        "roverMDP.actions (3) # ALREADY OUT OF THE GAME SO NOTHING but coding is easier this way"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xygEcf2Mp05M",
        "outputId": "5073b286-d61b-49df-e6e8-2d81205aa206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['slow', 'rapid']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# interpret what are the possible (new state, corresponding probability, corresponding reward)\n",
        "# available from state the 'low' state and action 'slow'\n",
        "roverMDP.NewStateProbReward ('low', 'slow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-8Ti10qGYZ",
        "outputId": "005d92c7-7c0e-4de0-a69e-96deb800bc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low', 0.7, -4), ('medium', 0.3, -4)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('low', 'rapid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziHixvnPr4m-",
        "outputId": "fc29d83a-4b03-467c-b3a3-56b60ae72093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low', 0.5, -12), ('medium', 0.5, -12)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('medium', 'slow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsuu1mlZr9e2",
        "outputId": "7a1bc406-d272-4e94-986d-82e1f35b81fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low', 0.7, -4), ('high', 0.3, -4)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('medium', 'rapid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBfv1cwjNp6v",
        "outputId": "0ec1707b-28d6-425b-b388-656f1d656f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low', 0.5, -12), ('high', 0.5, -12)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('high', 'slow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHI1fzYoNxRB",
        "outputId": "94c83055-4caa-4dea-b462-129959b7e2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low', 0.7, -4), ('top', 0.3, -4)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('high', 'rapid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uELWOieCN61V",
        "outputId": "05b2a99d-e4aa-45d5-fd3f-ccdce1147cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('low', 0.5, -12), ('top', 0.5, -12)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('top', 'slow')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm-TVUv4N9qf",
        "outputId": "babf0971-4482-42bf-b918-910496e740d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('top', 1.0, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roverMDP.NewStateProbReward ('top', 'rapid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBPocOaRN_o7",
        "outputId": "5f437170-ebe6-4881-a75f-b269cbda4854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('top', 1.0, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a deterministic policy (what action to take in a given state ?)\n",
        "# this case is if in states low or medium then choose rapid and if in state high then choose slow\n",
        "def policy(state):\n",
        "   if state in ['low', 'medium']:\n",
        "        return 'rapid'\n",
        "   elif state in ['high', 'top']:\n",
        "        return 'slow'"
      ],
      "metadata": {
        "id": "Rw8LMH3msGhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice ([0, 1], size = 1, p = [2/3, 1/3]) # out of 100 approximately, 0 will show up 66 times and 1 will show up 34 times"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_ld-LOfzbaI",
        "outputId": "c0e44f66-e2ce-4fc7-a9a6-4ed579d2f93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "range(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rdRjM9R1aId",
        "outputId": "e63ca999-7d03-4888-eac8-995e47799b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables\n",
        "G = 0  # Cumulative reward\n",
        "k = 0  # Timestamp (power of gamma)\n",
        "state = roverMDP.startState()  # Starting state\n",
        "\n",
        "# Generate reward sequence\n",
        "while not roverMDP.isEnd(state):\n",
        "    # Choose action based on the policy\n",
        "    action = policy(state)\n",
        "\n",
        "    # Get possible new states, probabilities, and rewards\n",
        "    SPR_list = roverMDP.NewStateProbReward(state, action)  # [(newState, prob, reward), ...]\n",
        "\n",
        "    # Extract transition probabilities\n",
        "    prob = [tup[1] for tup in SPR_list]\n",
        "\n",
        "    # Randomly select a new state based on transition probabilities\n",
        "    index = np.random.choice(range(len(SPR_list)), size=1, p=prob)[0]\n",
        "    newState, reward = SPR_list[index][0], SPR_list[index][2]  # New state and reward\n",
        "\n",
        "    # Print the step details\n",
        "    print(f'Current State = {state}, Action = {action}, New State = {newState}, Reward = {reward}')\n",
        "\n",
        "    # Update cumulative reward and state\n",
        "    G += (roverMDP.discount()) ** k * reward\n",
        "    state = newState\n",
        "    k += 1  # Increment the time step\n",
        "\n",
        "# Print the net discounted reward\n",
        "print(f'Net Discounted Reward = {G}')"
      ],
      "metadata": {
        "id": "U5Z0SZ7WxXKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d88421-8339-4a03-be94-49c592f63b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = high, Reward = -12\n",
            "Current State = high, Action = slow, New State = low, Reward = -4\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = high, Reward = -12\n",
            "Current State = high, Action = slow, New State = low, Reward = -4\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = high, Reward = -12\n",
            "Current State = high, Action = slow, New State = low, Reward = -4\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = high, Reward = -12\n",
            "Current State = high, Action = slow, New State = low, Reward = -4\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = high, Reward = -12\n",
            "Current State = high, Action = slow, New State = low, Reward = -4\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = low, Reward = -12\n",
            "Current State = low, Action = rapid, New State = medium, Reward = -12\n",
            "Current State = medium, Action = rapid, New State = high, Reward = -12\n",
            "Current State = high, Action = slow, New State = top, Reward = -4\n",
            "Net Discounted Reward = -203.52821608582823\n"
          ]
        }
      ]
    }
  ]
}